{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2fe46a2",
   "metadata": {},
   "source": [
    "### VLLM serve command:\n",
    "\n",
    "### Codestral-22B-v0.1\n",
    "``` sh\n",
    "docker run --rm --gpus all --privileged   --ipc=host  --shm-size=2g --network host -v ~/.cache/huggingface:/root/.cache/huggingface\n",
    "  -v /home/odedh/SML-For-Debug/models/:/models   vllm/vllm-openai:latest   --model /models/Codestral-22B-v0.1   --use-tqdm-on-load   \n",
    "--enable-chunked-prefill   --tensor-parallel-size 2\n",
    "\n",
    "```\n",
    "\n",
    "### Qwen3-0.6B\n",
    "``` sh\n",
    "docker run --rm --gpus all  --privileged   --ipc=host  --shm-size=2g --network host -v ~/.cache/huggingface:/root/.cache/huggingface -v /home/odedh/SML-For-Debug/models/:/models   -e CUDA_VISIBLE_DEVICES=2 vllm/vllm-openai:latest   --model /models/Qwen3-0.6B   --use-tqdm-on-load   --enable-chunked-prefill    --port 8001\n",
    "```\n",
    "\n",
    "\n",
    "### Llama-3.2-1B-Instruct\n",
    "``` sh\n",
    "docker run --rm --gpus all  --privileged   --ipc=host  --shm-size=2g --network host -v ~/.cache/huggingface:/root/.cache/huggingface -v /home/odedh/SML-For-Debug/models/:/models   -e CUDA_VISIBLE_DEVICES=2 vllm/vllm-openai:latest   --model /models/Llama-3.2-1B-Instruct   --use-tqdm-on-load   --enable-chunked-prefill    --port 8001\n",
    "\n",
    "```\n",
    "\n",
    "### GLM-4.5-Air\n",
    "``` sh\n",
    "docker run --rm --gpus all --privileged   --ipc=host  --shm-size=16g --network host -v ~/.cache/huggingface:/root/.cache/hggingface  -v /mnt/hf_models/:/models   vllm/vllm-openai:latest   --model /models/GLM-4.5-Air   --use-tqdm-on-load   --enable-chunked-prefill   --tensor-parallel-size=8  --port 8001\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ddf8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834487a",
   "metadata": {},
   "source": [
    "Running a simpel example for the Qwen3-0.6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac7a0d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/models/Qwen3-0.6B\n",
      "bug\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8001/v1\",\n",
    "    api_key=\"-\",\n",
    ")\n",
    "# print the model name\n",
    "print(client.models.list().data[0].id)\n",
    "\n",
    "model = client.models.list().data[0].id\n",
    "\n",
    "code_for_debugging = \"\"\"\n",
    "def divide_numbers(a, b):\n",
    "    return a / b\n",
    "\"\"\"\n",
    "content = f\"Classify this code and ans if there is a bug in it: {code_for_debugging}\"\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    temperature=0.1,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ],\n",
    "    extra_body={\"guided_choice\": [\"bug\", \"no-bug\"]},\n",
    "    \n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0957e9",
   "metadata": {},
   "source": [
    "Codestral-22B-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c97eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/models/Qwen3-0.6B\n",
      "<think>\n",
      "Okay, let's see. The user wants me to fix the code so that there are no bugs. The original code is:\n",
      "\n",
      "def divide_numbers(a, b):\n",
      "    return a / b\n",
      "\n",
      "And they want just the fixed version of the function, no explanation needed.\n",
      "\n",
      "First, I need to check if there are any syntax errors. The function is correctly defined with parameters a and b, and the return statement is a simple division. The code looks straightforward.\n",
      "\n",
      "Wait, maybe there's a problem with the return statement? Like, does it handle division by zero? But the user didn't mention that. They just want the function to return a / b. So perhaps the function is correct as is.\n",
      "\n",
      "Another thing to check: the function name is 'divide_numbers', which is a common name. The parameters are a and b, which are correct. The return value is a division. So no issues there.\n",
      "\n",
      "Is there any possible bug in the code? For example, if a or b is not a number, but the function is supposed to handle that. However, the problem statement doesn't mention handling non-numeric inputs. So maybe the function is correct.\n",
      "\n",
      "So the answer should be the same as the original code, but with the function definition. The user wants just the fixed version, so I need to present that.\n",
      "\n",
      "Wait, the user says \"just return the fixed version of the function, no explanation needed.\" So the code is already correct. So the answer is the same as the original code, but perhaps with the function definition as is.\n",
      "\n",
      "So the final answer is the same function as provided, but perhaps the user wants it to be enclosed in a code block. But the original code is already in a function definition. So the fixed version is the same.\n",
      "\n",
      "Therefore, the correct answer is the original code as written.\n",
      "</think>\n",
      "\n",
      "```python\n",
      "def divide_numbers(a, b):\n",
      "    return a / b\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8001/v1\",\n",
    "    api_key=\"-\",\n",
    "    \n",
    ")\n",
    "model = client.models.list().data[0].id\n",
    "print(model)\n",
    "\n",
    "code_for_debugging = \"\"\"\n",
    "def _make_sqlite_account_info(self, env=None, last_upgrade_to_run=None):\\n\\t\\twith mock.patch('os.environ', env or {'HOME': self.home}):\\n\\t\\t\\treturn SqliteAccountInfo(\\n\\t\\t\\t\\tfile_name=self.db_path if not env else None,\\n\\t\\t\\t\\tlast_upgrade_to_run=last_upgrade_to_run,\\n\\t\\t\\t)\n",
    "\"\"\"\n",
    "content = f\"Fix this code to have no bugs: {code_for_debugging}\\n\\n just return the fixed version of the function, no explanation needed\"\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    temperature=0.1,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ],\n",
    "    # extra_body={\"guided_choice\": [\"bug\", \"no-bug\"]},\n",
    "    \n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13b66b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"has_bug\": \"has_bug\", \"bug_type\": \"Missing If Construct Plus Statements (MIFS)\", \"fixability\": \"not_fixable\", \"tested_functions\": [\"get_api_response_ext\"] , \"rationale\": \"The function is missing an if statement to check if headers is None, which could lead to a TypeError when headers is None. This is a MIFS fault because the code does not include the if condition, and the fix is not possible to implement.\"}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from enum import Enum\n",
    "from typing import Optional, List\n",
    "\n",
    "class BugStatus(str, Enum):\n",
    "    no_bug = \"no_bug\"\n",
    "    has_bug = \"has_bug\"\n",
    "    # uncertain = \"uncertain\"\n",
    "\n",
    "class Fixability(str, Enum):\n",
    "    fixable = \"fixable\"\n",
    "    not_fixable = \"not_fixable\"\n",
    "    unknown = \"unknown\"\n",
    "\n",
    "class ClassificationResult(BaseModel):\n",
    "    has_bug: BugStatus\n",
    "    bug_type: Optional[str] = Field(default=None, description=\"Type of bug if present\")\n",
    "    suggested_fix: Optional[str] = Field(default=None, description=\"Optional fix attempt if model detects a bug\")\n",
    "    confidence_score: float = Field(default=0, ge=0.0, le=1.0)\n",
    "    fixability: Optional[Fixability] = None\n",
    "    tested_functions: Optional[List[str]] = None\n",
    "    rationale: Optional[str] = Field(default=None, description=\"Short reasoning for classification\")\n",
    "\n",
    "json_schema = ClassificationResult.model_json_schema()\n",
    "\n",
    "\n",
    "code_snippet = \"\"\"\n",
    "def get_api_response_ext(self, http_resp, url='/images', headers={},\\n\\t\\t\\t\\t\\t\\t\\t body=None, method=None, api=None,\\n\\t\\t\\t\\t\\t\\t\\t content_type=None):\\n\\t\\tif api is None:\\n\\t\\t\\tapi = self.api\\n\\t\\treq = webob.Request.blank(url)\\n\\t\\tfor k, v in headers.iteritems():\\n\\t\\t\\treq.headers[k] = v\\n\\t\\tif method:\\n\\t\\t\\treq.method = method\\n\\t\\tif body:\\n\\t\\t\\treq.body = body\\n\\t\\tif content_type == 'json':\\n\\t\\t\\treq.content_type = 'application/json'\\n\\t\\telif content_type == 'octet':\\n\\t\\t\\treq.content_type = 'application/octet-stream'\\n\\t\\tres = req.get_response(api)\\n\\t\\tself.assertEqual(res.status_int, http_resp)\\n\\t\\treturn res\n",
    "\"\"\"\n",
    "\n",
    "description = \"\"\"\n",
    "Modify the get_api_response_ext method to introduce a Missing If Construct Plus Statements (MIFS) fault. The function should fail due to removing the check if headers is None:, causing potential TypeError exceptions when headers is None and iteriting over it.\n",
    "\"\"\"\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=model,\n",
    "    temperature=0.15, # Lower temperature for more deterministic output\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are given a Python code snippet. Your job is to classify whether it contains a bug. \"\n",
    "                \"If it does, describe the bug and attempt a fix. If not, state that it is correct. \"\n",
    "                \"Return your response as a structured JSON matching the provided schema. \"\n",
    "                \"Do not explain or justify in natural language outside the JSON. \"\n",
    "                \"Do not assume the answer is known. Use your reasoning to classify the code.\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"The code snippet: {code_snippet}\\nShort description: {description}\"\n",
    "        }\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"code-diagnostic\",\n",
    "            \"schema\": json_schema\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
